name: Drupal Integration Tests

on:
  push:
    branches: [main, develop]
    paths:
      - 'src/drupal/**'
      - 'src/auth/**'
      - 'tests/integration/**'
      - 'package.json'
      - 'jest.config.json'
  pull_request:
    branches: [main, develop]
    paths:
      - 'src/drupal/**'
      - 'src/auth/**'
      - 'tests/integration/**'
      - 'package.json'
      - 'jest.config.json'
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch: # Allow manual triggering

jobs:
  integration-tests:
    name: Integration Tests (Node ${{ matrix.node-version }})
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: drupalize_mcp_test
          POSTGRES_HOST_AUTH_METHOD: trust
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    strategy:
      matrix:
        node-version: [18, 20]
        test-suite:
          - 'oauth-authentication-flow'
          - 'content-search-workflow'
          - 'content-transformation-pipeline'
          - 'schema-discovery-translation'
          - 'error-handling-recovery'
          - 'performance-reliability-scenarios'
          - 'drupal-integration-suite'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Setup test database
        run: |
          # Wait for PostgreSQL to be ready
          until pg_isready -h localhost -p 5432; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done

          # Create extensions
          PGPASSWORD=test psql -h localhost -U test -d drupalize_mcp_test -c "
            CREATE EXTENSION IF NOT EXISTS pgcrypto;
            CREATE EXTENSION IF NOT EXISTS uuid-ossp;
          "
        env:
          PGPASSWORD: test

      - name: Run database migrations
        run: npm run db:test
        env:
          TEST_DATABASE_URL: postgresql://test:test@localhost:5432/drupalize_mcp_test

      - name: Run type checking
        run: npm run type-check

      - name: Run integration tests
        run: npm test tests/integration/${{ matrix.test-suite }}.test.ts -- --coverage --coverageDirectory=coverage/${{ matrix.test-suite }}
        env:
          NODE_ENV: test
          TEST_DATABASE_URL: postgresql://test:test@localhost:5432/drupalize_mcp_test
          TEST_REDIS_URL: redis://localhost:6379
          CI: true
          JEST_TIMEOUT: 30000
          LOG_LEVEL: error

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-node${{ matrix.node-version }}-${{ matrix.test-suite }}
          path: |
            coverage/
            test-results/
            junit.xml
          retention-days: 7

      - name: Upload coverage to Codecov
        if: matrix.node-version == 20
        uses: codecov/codecov-action@v4
        with:
          files: coverage/${{ matrix.test-suite }}/lcov.info
          flags: integration-tests,${{ matrix.test-suite }}
          name: integration-${{ matrix.test-suite }}
          token: ${{ secrets.CODECOV_TOKEN }}
          fail_ci_if_error: false

  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: integration-tests
    if: always()

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: drupalize_mcp_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Setup test database
        run: |
          until pg_isready -h localhost -p 5432; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done

          PGPASSWORD=test psql -h localhost -U test -d drupalize_mcp_test -c "
            CREATE EXTENSION IF NOT EXISTS pgcrypto;
            CREATE EXTENSION IF NOT EXISTS uuid-ossp;
          "
        env:
          PGPASSWORD: test

      - name: Run database migrations
        run: npm run db:test
        env:
          TEST_DATABASE_URL: postgresql://test:test@localhost:5432/drupalize_mcp_test

      - name: Run performance tests
        run: npm test tests/integration/performance-reliability-scenarios.test.ts -- --verbose
        env:
          NODE_ENV: test
          TEST_DATABASE_URL: postgresql://test:test@localhost:5432/drupalize_mcp_test
          CI: true
          PERFORMANCE_TESTING: true

      - name: Download baseline performance data
        id: download-baseline
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: performance-baseline
          path: .github/

      - name: Compare performance metrics
        run: |
          if [ -f ".github/performance-baseline.json" ]; then
            echo "Comparing against baseline..."
            npm run test:performance:compare .github/performance-baseline.json
          else
            echo "No baseline found, creating new baseline..."
            cp performance-metrics.json .github/performance-baseline.json
          fi
        continue-on-error: true

      - name: Upload performance metrics
        uses: actions/upload-artifact@v4
        with:
          name: performance-metrics
          path: |
            performance-metrics.json
            .github/performance-baseline.json
          retention-days: 30

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [integration-tests, performance-analysis]
    if: always()

    steps:
      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-*
          path: test-results/
          merge-multiple: true

      - name: Generate test summary
        run: |
          echo "# Integration Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count test results
          TOTAL_TESTS=0
          PASSED_TESTS=0
          FAILED_TESTS=0

          for result_file in test-results/*/junit.xml; do
            if [ -f "$result_file" ]; then
              TESTS=$(grep -o 'tests="[0-9]*"' "$result_file" | grep -o '[0-9]*' | head -1)
              FAILURES=$(grep -o 'failures="[0-9]*"' "$result_file" | grep -o '[0-9]*' | head -1)
              ERRORS=$(grep -o 'errors="[0-9]*"' "$result_file" | grep -o '[0-9]*' | head -1)
              
              TOTAL_TESTS=$((TOTAL_TESTS + ${TESTS:-0}))
              FAILED_TESTS=$((FAILED_TESTS + ${FAILURES:-0} + ${ERRORS:-0}))
            fi
          done

          PASSED_TESTS=$((TOTAL_TESTS - FAILED_TESTS))

          echo "## Overall Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Tests:** $TOTAL_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed:** $PASSED_TESTS âœ…" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed:** $FAILED_TESTS âŒ" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Success rate
          if [ $TOTAL_TESTS -gt 0 ]; then
            SUCCESS_RATE=$(echo "scale=2; $PASSED_TESTS * 100 / $TOTAL_TESTS" | bc)
            echo "- **Success Rate:** ${SUCCESS_RATE}%" >> $GITHUB_STEP_SUMMARY
          fi

          # Test matrix results
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Matrix Results" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Node 18 | Node 20 |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|---------|---------|" >> $GITHUB_STEP_SUMMARY

          for suite in oauth-authentication-flow content-search-workflow content-transformation-pipeline schema-discovery-translation error-handling-recovery performance-reliability-scenarios drupal-integration-suite; do
            echo -n "| $suite |" >> $GITHUB_STEP_SUMMARY
            
            for node in 18 20; do
              if [ -f "test-results/test-results-node${node}-${suite}/junit.xml" ]; then
                SUITE_FAILURES=$(grep -o 'failures="[0-9]*"' "test-results/test-results-node${node}-${suite}/junit.xml" | grep -o '[0-9]*' | head -1)
                SUITE_ERRORS=$(grep -o 'errors="[0-9]*"' "test-results/test-results-node${node}-${suite}/junit.xml" | grep -o '[0-9]*' | head -1)
                
                if [ "${SUITE_FAILURES:-0}" -eq 0 ] && [ "${SUITE_ERRORS:-0}" -eq 0 ]; then
                  echo -n " âœ… |" >> $GITHUB_STEP_SUMMARY
                else
                  echo -n " âŒ |" >> $GITHUB_STEP_SUMMARY
                fi
              else
                echo -n " âš ï¸ |" >> $GITHUB_STEP_SUMMARY
              fi
            done
            
            echo "" >> $GITHUB_STEP_SUMMARY
          done

          # Set job status
          if [ $FAILED_TESTS -gt 0 ]; then
            echo "TEST_STATUS=failure" >> $GITHUB_ENV
            exit 1
          else
            echo "TEST_STATUS=success" >> $GITHUB_ENV
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync(process.env.GITHUB_STEP_SUMMARY, 'utf8');

            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ðŸ§ª Integration Test Results\n\n${summary}`
            });

  notify-slack:
    name: Notify Slack
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: failure() && github.ref == 'refs/heads/main'

    steps:
      - name: Send Slack notification
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          text: |
            ðŸš¨ Drupal Integration Tests Failed on main branch

            â€¢ Commit: ${{ github.sha }}
            â€¢ Author: ${{ github.actor }}
            â€¢ Workflow: ${{ github.workflow }}

            Please check the test results and fix any failing tests.
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  cleanup:
    name: Cleanup
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: always()

    steps:
      - name: Clean up old artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.runId,
            });

            // Keep only the most recent 10 performance baselines
            const performanceArtifacts = artifacts.data.artifacts
              .filter(artifact => artifact.name.includes('performance'))
              .sort((a, b) => new Date(b.created_at) - new Date(a.created_at))
              .slice(10);

            for (const artifact of performanceArtifacts) {
              await github.rest.actions.deleteArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: artifact.id,
              });
            }
